{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee87e18-89bf-4f77-a200-760c891447dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from diffusers import DDPMScheduler, AutoencoderKL\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy, _apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from typing import List\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download and cache model locally\n",
    "model_path = snapshot_download(repo_id=\"yisol/IDM-VTON\", local_dir=\"saved_models/IDM-VTON\")\n",
    "\n",
    "# Set paths\n",
    "base_path = 'saved_models/IDM-VTON'\n",
    "example_path = os.path.join(os.getcwd(), 'images')\n",
    "\n",
    "# Load models\n",
    "unet = UNet2DConditionModel.from_pretrained(base_path, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "\n",
    "unet.requires_grad_(False)\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(base_path, subfolder=\"tokenizer\", use_fast=False)\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(base_path, subfolder=\"tokenizer_2\", use_fast=False)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(base_path, subfolder=\"scheduler\")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(base_path, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
    "text_encoder_two = CLIPTextModelWithProjection.from_pretrained(base_path, subfolder=\"text_encoder_2\", torch_dtype=torch.float16)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(base_path, subfolder=\"image_encoder\", torch_dtype=torch.float16)\n",
    "vae = AutoencoderKL.from_pretrained(base_path, subfolder=\"vae\", torch_dtype=torch.float16)\n",
    "UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(base_path, subfolder=\"unet_encoder\", torch_dtype=torch.float16)\n",
    "\n",
    "# Disable gradient computation\n",
    "UNet_Encoder.requires_grad_(False)\n",
    "image_encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "tensor_transfrom = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize parsing models\n",
    "parsing_model = Parsing(0)\n",
    "openpose_model = OpenPose(0)\n",
    "\n",
    "# Load Try-On pipeline\n",
    "pipe = TryonPipeline.from_pretrained(\n",
    "    base_path,\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    feature_extractor=CLIPImageProcessor(),\n",
    "    text_encoder=text_encoder_one,\n",
    "    text_encoder_2=text_encoder_two,\n",
    "    tokenizer=tokenizer_one,\n",
    "    tokenizer_2=tokenizer_two,\n",
    "    scheduler=noise_scheduler,\n",
    "    image_encoder=image_encoder,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.unet_encoder = UNet_Encoder\n",
    "\n",
    "# Load example images\n",
    "human_img_path = os.path.join(example_path, \"human\", \"human.jpg\")\n",
    "garment_img_path = os.path.join(example_path, \"cloth\", \"cloth.jpg\")\n",
    "\n",
    "# Ensure input is on the same device\n",
    "human_img = Image.open(human_img_path).convert(\"RGB\").resize((768, 1024))\n",
    "garm_img = Image.open(garment_img_path).convert(\"RGB\").resize((768, 1024))\n",
    "\n",
    "# Preprocessing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "openpose_model.preprocessor.body_estimation.model.to(device)\n",
    "\n",
    "pipe.to(device)\n",
    "pipe.unet_encoder.to(device)\n",
    "\n",
    "keypoints = openpose_model(human_img.resize((384,512)))\n",
    "model_parse, _ = parsing_model(human_img.resize((384, 512)))\n",
    "mask, mask_gray = get_mask_location(\"hd\", \"upper_body\", model_parse, keypoints)\n",
    "mask = mask.resize((768, 1024))\n",
    "\n",
    "mask_gray = (1-transforms.ToTensor()(mask)) * tensor_transfrom(human_img)\n",
    "mask_gray = to_pil_image((mask_gray+1.0)/2.0)\n",
    "\n",
    "human_img_arg = _apply_exif_orientation(human_img.resize((384,512)))\n",
    "human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "\n",
    "args = apply_net.create_argument_parser().parse_args(('show', './configs/densepose_rcnn_R_50_FPN_s1x.yaml', './ckpt/densepose/model_final_162be9.pkl', 'dp_segm', '-v', '--opts', 'MODEL.DEVICE', 'cuda'))\n",
    "# verbosity = getattr(args, \"verbosity\", None)\n",
    "pose_img = args.func(args,human_img_arg)    \n",
    "pose_img = pose_img[:,:,::-1]    \n",
    "pose_img = Image.fromarray(pose_img).resize((768,1024))\n",
    "\n",
    "# Generate try-on result\n",
    "with torch.no_grad():\n",
    "    # Extract the images\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            prompt = \"model is wearing Description of garment ex) Short Sleeve Round Neck T-shirts\"\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "            with torch.inference_mode():\n",
    "                (\n",
    "                    prompt_embeds,\n",
    "                    negative_prompt_embeds,\n",
    "                    pooled_prompt_embeds,\n",
    "                    negative_pooled_prompt_embeds,\n",
    "                ) = pipe.encode_prompt(\n",
    "                    prompt,\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=True,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                )\n",
    "                                    \n",
    "                prompt = \"a photo of Description of garment ex) Short Sleeve Round Neck T-shirts\"\n",
    "                negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "                if not isinstance(prompt, List):\n",
    "                    prompt = [prompt] * 1\n",
    "                if not isinstance(negative_prompt, List):\n",
    "                    negative_prompt = [negative_prompt] * 1\n",
    "                with torch.inference_mode():\n",
    "                    (\n",
    "                        prompt_embeds_c,\n",
    "                        _,\n",
    "                        _,\n",
    "                        _,\n",
    "                    ) = pipe.encode_prompt(\n",
    "                        prompt,\n",
    "                        num_images_per_prompt=1,\n",
    "                        do_classifier_free_guidance=False,\n",
    "                        negative_prompt=negative_prompt,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "                pose_img =  tensor_transfrom(pose_img).unsqueeze(0).to(device,torch.float16)\n",
    "                garm_tensor =  tensor_transfrom(garm_img).unsqueeze(0).to(device,torch.float16)\n",
    "                generator = torch.Generator(device).manual_seed(42)\n",
    "                images = pipe(\n",
    "                    prompt_embeds=prompt_embeds.to(device,torch.float16),\n",
    "                    negative_prompt_embeds=negative_prompt_embeds.to(device,torch.float16),\n",
    "                    pooled_prompt_embeds=pooled_prompt_embeds.to(device,torch.float16),\n",
    "                    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device,torch.float16),\n",
    "                    num_inference_steps=30,\n",
    "                    generator=generator,\n",
    "                    strength = 1.0,\n",
    "                    pose_img = pose_img.to(device,torch.float16),\n",
    "                    text_embeds_cloth=prompt_embeds_c.to(device,torch.float16),\n",
    "                    cloth = garm_tensor.to(device,torch.float16),\n",
    "                    mask_image=mask,\n",
    "                    image=human_img, \n",
    "                    height=1024,\n",
    "                    width=768,\n",
    "                    ip_adapter_image = garm_img.resize((768,1024)),\n",
    "                    guidance_scale=2.0,\n",
    "                )[0]\n",
    "\n",
    "    output_path = os.path.join(example_path, \"output_tryon.png\")\n",
    "    images[0].save(output_path)\n",
    "    print(f\"Generated try-on image saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
