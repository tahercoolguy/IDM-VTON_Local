{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/camenduru/IDM-VTON/resolve/main/humanparsing/parsing_atr.onnx -O /workspace/IDM-VTON/ckpt/humanparsing/parsing_atr.onnx\n",
    "!wget https://huggingface.co/camenduru/IDM-VTON/resolve/main/densepose/model_final_162be9.pkl -O /workspace/IDM-VTON/ckpt/densepose/model_final_162be9.pkl\n",
    "\n",
    "!wget https://huggingface.co/camenduru/IDM-VTON/resolve/main/humanparsing/parsing_lip.onnx -O /workspace/IDM-VTON/ckpt/humanparsing/parsing_lip.onnx\n",
    "!wget https://huggingface.co/camenduru/IDM-VTON/resolve/main/openpose/ckpts/body_pose_model.pth -O /workspace/IDM-VTON/ckpt/openpose/ckpts/body_pose_model.pth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In start install ysol repo of hugging face as mentioned then install all requirements of camenduru as it was perfect. then run above cell to download all variables. One more things huggingface_hub should be 0.25.0 because there was error in it . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee87e18-89bf-4f77-a200-760c891447dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from diffusers import DDPMScheduler, AutoencoderKL\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy, _apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from typing import List\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download and cache model locally\n",
    "model_path = snapshot_download(repo_id=\"yisol/IDM-VTON\", local_dir=\"saved_models/IDM-VTON\")\n",
    "\n",
    "# Set paths\n",
    "base_path = 'saved_models/IDM-VTON'\n",
    "example_path = os.path.join(os.getcwd(), 'example')\n",
    "\n",
    "# Load models\n",
    "unet = UNet2DConditionModel.from_pretrained(base_path, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "\n",
    "unet.requires_grad_(False)\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(base_path, subfolder=\"tokenizer\", use_fast=False)\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(base_path, subfolder=\"tokenizer_2\", use_fast=False)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(base_path, subfolder=\"scheduler\")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(base_path, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
    "text_encoder_two = CLIPTextModelWithProjection.from_pretrained(base_path, subfolder=\"text_encoder_2\", torch_dtype=torch.float16)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(base_path, subfolder=\"image_encoder\", torch_dtype=torch.float16)\n",
    "vae = AutoencoderKL.from_pretrained(base_path, subfolder=\"vae\", torch_dtype=torch.float16)\n",
    "UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(base_path, subfolder=\"unet_encoder\", torch_dtype=torch.float16)\n",
    "\n",
    "# Disable gradient computation\n",
    "UNet_Encoder.requires_grad_(False)\n",
    "image_encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "tensor_transfrom = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize parsing models\n",
    "parsing_model = Parsing(0)\n",
    "openpose_model = OpenPose(0)\n",
    "\n",
    "# Load Try-On pipeline\n",
    "pipe = TryonPipeline.from_pretrained(\n",
    "    base_path,\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    feature_extractor=CLIPImageProcessor(),\n",
    "    text_encoder=text_encoder_one,\n",
    "    text_encoder_2=text_encoder_two,\n",
    "    tokenizer=tokenizer_one,\n",
    "    tokenizer_2=tokenizer_two,\n",
    "    scheduler=noise_scheduler,\n",
    "    image_encoder=image_encoder,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.unet_encoder = UNet_Encoder\n",
    "\n",
    "# Load example images\n",
    "human_img_path = os.path.join(example_path, \"human\", \"human.jpg\")\n",
    "garment_img_path = os.path.join(example_path, \"cloth\", \"cloth.jpg\")\n",
    "\n",
    "# Ensure input is on the same device\n",
    "human_img = Image.open(human_img_path).convert(\"RGB\").resize((768, 1024))\n",
    "garm_img = Image.open(garment_img_path).convert(\"RGB\").resize((768, 1024))\n",
    "\n",
    "# Preprocessing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "openpose_model.preprocessor.body_estimation.model.to(device)\n",
    "\n",
    "pipe.to(device)\n",
    "pipe.unet_encoder.to(device)\n",
    "\n",
    "keypoints = openpose_model(human_img.resize((384,512)))\n",
    "model_parse, _ = parsing_model(human_img.resize((384, 512)))\n",
    "mask, mask_gray = get_mask_location(\"hd\", \"upper_body\", model_parse, keypoints)\n",
    "mask = mask.resize((768, 1024))\n",
    "\n",
    "mask_gray = (1-transforms.ToTensor()(mask)) * tensor_transfrom(human_img)\n",
    "mask_gray = to_pil_image((mask_gray+1.0)/2.0)\n",
    "\n",
    "human_img_arg = _apply_exif_orientation(human_img.resize((384,512)))\n",
    "human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "\n",
    "args = apply_net.create_argument_parser().parse_args(('show', './configs/densepose_rcnn_R_50_FPN_s1x.yaml', './ckpt/densepose/model_final_162be9.pkl', 'dp_segm', '-v', '--opts', 'MODEL.DEVICE', 'cuda'))\n",
    "# verbosity = getattr(args, \"verbosity\", None)\n",
    "pose_img = args.func(args,human_img_arg)    \n",
    "pose_img = pose_img[:,:,::-1]    \n",
    "pose_img = Image.fromarray(pose_img).resize((768,1024))\n",
    "\n",
    "# Generate try-on result\n",
    "with torch.no_grad():\n",
    "    # Extract the images\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            prompt = \"model is wearing Description of garment ex) Short Sleeve Round Neck T-shirts\"\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "            with torch.inference_mode():\n",
    "                (\n",
    "                    prompt_embeds,\n",
    "                    negative_prompt_embeds,\n",
    "                    pooled_prompt_embeds,\n",
    "                    negative_pooled_prompt_embeds,\n",
    "                ) = pipe.encode_prompt(\n",
    "                    prompt,\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=True,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                )\n",
    "                                    \n",
    "                prompt = \"a photo of Description of garment ex) Short Sleeve Round Neck T-shirts\"\n",
    "                negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "                if not isinstance(prompt, List):\n",
    "                    prompt = [prompt] * 1\n",
    "                if not isinstance(negative_prompt, List):\n",
    "                    negative_prompt = [negative_prompt] * 1\n",
    "                with torch.inference_mode():\n",
    "                    (\n",
    "                        prompt_embeds_c,\n",
    "                        _,\n",
    "                        _,\n",
    "                        _,\n",
    "                    ) = pipe.encode_prompt(\n",
    "                        prompt,\n",
    "                        num_images_per_prompt=1,\n",
    "                        do_classifier_free_guidance=False,\n",
    "                        negative_prompt=negative_prompt,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "                pose_img =  tensor_transfrom(pose_img).unsqueeze(0).to(device,torch.float16)\n",
    "                garm_tensor =  tensor_transfrom(garm_img).unsqueeze(0).to(device,torch.float16)\n",
    "                generator = torch.Generator(device).manual_seed(42)\n",
    "                images = pipe(\n",
    "                    prompt_embeds=prompt_embeds.to(device,torch.float16),\n",
    "                    negative_prompt_embeds=negative_prompt_embeds.to(device,torch.float16),\n",
    "                    pooled_prompt_embeds=pooled_prompt_embeds.to(device,torch.float16),\n",
    "                    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device,torch.float16),\n",
    "                    num_inference_steps=30,\n",
    "                    generator=generator,\n",
    "                    strength = 1.0,\n",
    "                    pose_img = pose_img.to(device,torch.float16),\n",
    "                    text_embeds_cloth=prompt_embeds_c.to(device,torch.float16),\n",
    "                    cloth = garm_tensor.to(device,torch.float16),\n",
    "                    mask_image=mask,\n",
    "                    image=human_img, \n",
    "                    height=1024,\n",
    "                    width=768,\n",
    "                    ip_adapter_image = garm_img.resize((768,1024)),\n",
    "                    guidance_scale=2.0,\n",
    "                )[0]\n",
    "\n",
    "    output_path = os.path.join(example_path, \"output_tryon.png\")\n",
    "    images[0].save(output_path)\n",
    "    print(f\"Generated try-on image saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
